{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEANYSVuiIlA"
      },
      "source": [
        "# **Instruções de Uso:** Clique em \"Ambiente de execução\" depois em \"Executar tudo\" (CTRL + F9), e aguarde até que a interface do Gradio apareça.\n",
        "\n",
        "# Na interface, selecione a aba que deseja utilizar, gerar o modelo através de uma imagem, ou através de um prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZj2lnD7XhYp"
      },
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZSs980qX1-B8"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install onnxruntime\n",
        "!pip install rembg\n",
        "!pip install trimesh\n",
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "thpVR5252BM7"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/tencent/Hunyuan3D-2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vR_CB4Pc3IX-"
      },
      "outputs": [],
      "source": [
        "%cd Hunyuan3D-2\n",
        "!pip install -r requirements.txt\n",
        "!cd hy3dgen/texgen/custom_rasterizer && pip install .\n",
        "!cd hy3dgen/texgen/differentiable_renderer && bash compile_mesh_painter.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ecR9tWifnLkg"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.29.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Msam9shU24xC"
      },
      "outputs": [],
      "source": [
        "from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline\n",
        "\n",
        "pipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained('tencent/Hunyuan3D-2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoAL9kHXUcbE"
      },
      "source": [
        "# GERAR MODELO 3D COM BASE EM UMA IMAGEM EXISTENTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiG6BCsGb69a"
      },
      "outputs": [],
      "source": [
        "from rembg import remove\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "def generate_stl_from_image(img, format=\".glb\"):\n",
        "\n",
        "    if isinstance(img, str):\n",
        "        img = Image.open(img)\n",
        "        img = np.array(img)\n",
        "\n",
        "    img = Image.fromarray(img)\n",
        "    img_sem_fundo = remove(img)\n",
        "\n",
        "    mesh = pipeline(image=img_sem_fundo)[0]\n",
        "    file_path_visu = \"output_image_visu.glb\"\n",
        "    file_path = \"output_image\"+format\n",
        "\n",
        "    # Export file to visualization\n",
        "    mesh.export(file_path_visu)\n",
        "    # Export file to download\n",
        "    mesh.export(file_path)\n",
        "\n",
        "    return file_path_visu, file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYW4wzlmU6Al"
      },
      "source": [
        "# GERAR MODELO 3D COM BASE NUM PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jvHDYIcqU2Pn"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b totoro3 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.28.post2\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8.safetensors -d /content/TotoroUI/models/unet -o flux1-dev-fp8.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/TotoroUI/models/vae -o ae.sft\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d /content/TotoroUI/models/clip -o clip_l.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors -d /content/TotoroUI/models/clip -o t5xxl_fp8_e4m3fn.safetensors\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from totoro_extras import nodes_custom_sampler\n",
        "from totoro import model_management\n",
        "from rembg import remove\n",
        "\n",
        "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
        "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
        "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
        "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
        "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
        "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
        "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
        "    unet = UNETLoader.load_unet(\"flux1-dev-fp8.safetensors\", \"fp8_e4m3fn\")[0]\n",
        "    vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
        "\n",
        "def closestNumber(n, m):\n",
        "    q = int(n / m)\n",
        "    n1 = m * q\n",
        "    if (n * m) > 0:\n",
        "        n2 = m * (q + 1)\n",
        "    else:\n",
        "        n2 = m * (q - 1)\n",
        "    if abs(n - n1) < abs(n - n2):\n",
        "        return n1\n",
        "    return n2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2zED8CrWTxx"
      },
      "outputs": [],
      "source": [
        "def generate_image(prompt, format=\".glb\"):\n",
        "  with torch.inference_mode():\n",
        "    positive_prompt = prompt\n",
        "    width = 720\n",
        "    height = 720\n",
        "    seed = 0\n",
        "    steps = 20\n",
        "    sampler_name = \"euler\"\n",
        "    scheduler = \"simple\"\n",
        "\n",
        "    if seed == 0:\n",
        "        seed = random.randint(0, 18446744073709551615)\n",
        "    print(seed)\n",
        "\n",
        "    cond, pooled = clip.encode_from_tokens(clip.tokenize(positive_prompt), return_pooled=True)\n",
        "    cond = [[cond, {\"pooled_output\": pooled}]]\n",
        "    noise = RandomNoise.get_noise(seed)[0]\n",
        "    guider = BasicGuider.get_guider(unet, cond)[0]\n",
        "    sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
        "    sigmas = BasicScheduler.get_sigmas(unet, scheduler, steps, 1.0)[0]\n",
        "    latent_image = EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16))[0]\n",
        "    sample, sample_denoised = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
        "    model_management.soft_empty_cache()\n",
        "    decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "    Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(\"/content/flux.png\")\n",
        "\n",
        "  imagem = Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0])\n",
        "\n",
        "  img_sem_fundo = remove(imagem)\n",
        "\n",
        "  mesh = pipeline(image=img_sem_fundo)[0]\n",
        "  file_path_visu = \"output_image_visu.glb\"\n",
        "  file_path = \"output_image\"+format\n",
        "\n",
        "  # Export file to visualization\n",
        "  mesh.export(file_path_visu)\n",
        "  # Export file to download\n",
        "  mesh.export(file_path)\n",
        "\n",
        "  return file_path_visu, file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1aqs-s-AJbd"
      },
      "source": [
        "# Image Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXklKelwOpaE"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/TencentARC/PhotoMaker.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AfErSCSeOONn"
      },
      "outputs": [],
      "source": [
        "%cd PhotoMaker\n",
        "!pip install -r requirements.txt\n",
        "!pip install git+https://github.com/TencentARC/PhotoMaker.git\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I63plCaUAJOr"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "import os\n",
        "\n",
        "from diffusers.utils import load_image\n",
        "from diffusers import EulerDiscreteScheduler, T2IAdapter\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "import spaces\n",
        "import gradio as gr\n",
        "\n",
        "from photomaker import PhotoMakerStableDiffusionXLAdapterPipeline\n",
        "from photomaker import FaceAnalysis2, analyze_faces\n",
        "import torch\n",
        "\n",
        "# global variable\n",
        "base_model_path = 'SG161222/RealVisXL_V4.0'\n",
        "face_detector = FaceAnalysis2(providers=['CUDAExecutionProvider'], allowed_modules=['detection', 'recognition'])\n",
        "face_detector.prepare(ctx_id=0, det_size=(640, 640))\n",
        "\n",
        "try:\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "except:\n",
        "    device = \"cpu\"\n",
        "\n",
        "\n",
        "DEFAULT_STYLE_NAME = \"Photographic (Default)\"\n",
        "\n",
        "\n",
        "enable_doodle_arg = False\n",
        "photomaker_ckpt = hf_hub_download(repo_id=\"TencentARC/PhotoMaker-V2\", filename=\"photomaker-v2.bin\", repo_type=\"model\")\n",
        "\n",
        "torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "if device == \"mps\":\n",
        "    torch_dtype = torch.float16\n",
        "\n",
        "# load adapter\n",
        "adapter = T2IAdapter.from_pretrained(\n",
        "    \"TencentARC/t2i-adapter-sketch-sdxl-1.0\", torch_dtype=torch_dtype, variant=\"fp16\"\n",
        ").to(device)\n",
        "\n",
        "pipe = PhotoMakerStableDiffusionXLAdapterPipeline.from_pretrained(\n",
        "    base_model_path,\n",
        "    adapter=adapter,\n",
        "    torch_dtype=torch_dtype,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\",\n",
        ").to(device)\n",
        "\n",
        "pipe.load_photomaker_adapter(\n",
        "    os.path.dirname(photomaker_ckpt),\n",
        "    subfolder=\"\",\n",
        "    weight_name=os.path.basename(photomaker_ckpt),\n",
        "    trigger_word=\"img\",\n",
        "    pm_version=\"v2\",\n",
        ")\n",
        "pipe.id_encoder.to(device)\n",
        "\n",
        "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "# pipe.set_adapters([\"photomaker\"], adapter_weights=[1.0])\n",
        "pipe.fuse_lora()\n",
        "pipe.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwNcCg5CdEb4"
      },
      "outputs": [],
      "source": [
        "#Define styles:\n",
        "style_list = [\n",
        "    {\n",
        "        \"name\": \"(No style)\",\n",
        "        \"prompt\": \"{prompt}\",\n",
        "        \"negative_prompt\": \"\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Cinematic\",\n",
        "        \"prompt\": \"cinematic still {prompt} . emotional, harmonious, vignette, highly detailed, high budget, bokeh, cinemascope, moody, epic, gorgeous, film grain, grainy\",\n",
        "        \"negative_prompt\": \"anime, cartoon, graphic, text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Disney Charactor\",\n",
        "        \"prompt\": \"A Pixar animation character of {prompt} . pixar-style, studio anime, Disney, high-quality\",\n",
        "        \"negative_prompt\": \"lowres, bad anatomy, bad hands, text, bad eyes, bad arms, bad legs, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, blurry, grayscale, noisy, sloppy, messy, grainy, highly detailed, ultra textured, photo\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Digital Art\",\n",
        "        \"prompt\": \"concept art {prompt} . digital artwork, illustrative, painterly, matte painting, highly detailed\",\n",
        "        \"negative_prompt\": \"photo, photorealistic, realism, ugly\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Photographic (Default)\",\n",
        "        \"prompt\": \"cinematic photo {prompt} . 35mm photograph, film, bokeh, professional, 4k, highly detailed\",\n",
        "        \"negative_prompt\": \"drawing, painting, crayon, sketch, graphite, impressionist, noisy, blurry, soft, deformed, ugly\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Fantasy art\",\n",
        "        \"prompt\": \"ethereal fantasy concept art of  {prompt} . magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy\",\n",
        "        \"negative_prompt\": \"photographic, realistic, realism, 35mm film, dslr, cropped, frame, text, deformed, glitch, noise, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, sloppy, duplicate, mutated, black and white\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Neonpunk\",\n",
        "        \"prompt\": \"neonpunk style {prompt} . cyberpunk, vaporwave, neon, vibes, vibrant, stunningly beautiful, crisp, detailed, sleek, ultramodern, magenta highlights, dark purple shadows, high contrast, cinematic, ultra detailed, intricate, professional\",\n",
        "        \"negative_prompt\": \"painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Enhance\",\n",
        "        \"prompt\": \"breathtaking {prompt} . award-winning, professional, highly detailed\",\n",
        "        \"negative_prompt\": \"ugly, deformed, noisy, blurry, distorted, grainy\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Comic book\",\n",
        "        \"prompt\": \"comic {prompt} . graphic illustration, comic art, graphic novel art, vibrant, highly detailed\",\n",
        "        \"negative_prompt\": \"photograph, deformed, glitch, noisy, realistic, stock photo\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Lowpoly\",\n",
        "        \"prompt\": \"low-poly style {prompt} . low-poly game art, polygon mesh, jagged, blocky, wireframe edges, centered composition\",\n",
        "        \"negative_prompt\": \"noisy, sloppy, messy, grainy, highly detailed, ultra textured, photo\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Line art\",\n",
        "        \"prompt\": \"line art drawing {prompt} . professional, sleek, modern, minimalist, graphic, line art, vector graphics\",\n",
        "        \"negative_prompt\": \"anime, photorealistic, 35mm film, deformed, glitch, blurry, noisy, off-center, deformed, cross-eyed, closed eyes, bad anatomy, ugly, disfigured, mutated, realism, realistic, impressionism, expressionism, oil, acrylic\",\n",
        "    }\n",
        "]\n",
        "\n",
        "styles = {k[\"name\"]: (k[\"prompt\"], k[\"negative_prompt\"]) for k in style_list}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1r_g7aalomE"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "aspect_ratios = [640, 1024]\n",
        "\n",
        "DEFAULT_STYLE_NAME = \"Photographic (Default)\"\n",
        "\n",
        "def apply_style(style_name: str, positive: str, negative: str = \"\"):\n",
        "    p, n = styles.get(style_name, styles[DEFAULT_STYLE_NAME])\n",
        "    return p.replace(\"{prompt}\", positive), n + ' ' + negative\n",
        "\n",
        "def generate_style_image_v2(\n",
        "        upload_images,\n",
        "        prompt,\n",
        "        negative_prompt,\n",
        "        style_name,\n",
        "        num_steps,\n",
        "        style_strength_ratio,\n",
        "        num_outputs,\n",
        "        guidance_scale,\n",
        "        seed,\n",
        "        progress=gr.Progress(track_tqdm=True)):\n",
        "\n",
        "    # check the trigger word\n",
        "    image_token_id = pipe.tokenizer.convert_tokens_to_ids(pipe.trigger_word)\n",
        "    # prompt = prompt + \", face only portrait,\"\n",
        "\n",
        "    input_ids = pipe.tokenizer.encode(prompt)\n",
        "    if image_token_id not in input_ids:\n",
        "        raise gr.Error(f\"Cannot find the trigger word '{pipe.trigger_word}' in text prompt!\")\n",
        "\n",
        "    if input_ids.count(image_token_id) > 1:\n",
        "        raise gr.Error(f\"Cannot use multiple trigger words '{pipe.trigger_word}' in text prompt!\")\n",
        "\n",
        "    # determine output dimensions by the aspect ratio\n",
        "    aspect_ratio_name=\"35mm film / Portrait (2:3)\"\n",
        "    output_w, output_h = aspect_ratios\n",
        "    print(f\"[Debug] Generate image using aspect ratio [{aspect_ratio_name}] => {output_w} x {output_h}\")\n",
        "\n",
        "    # apply the style template\n",
        "    prompt, negative_prompt = apply_style(style_name, prompt, negative_prompt)\n",
        "\n",
        "    print(f\"[DEBUG] Image path: {upload_images}\")\n",
        "\n",
        "    if upload_images is None:\n",
        "        raise gr.Error(f\"Cannot find any input face image!\")\n",
        "\n",
        "    input_id_images = []\n",
        "    for img in upload_images:\n",
        "        input_id_images.append(load_image(img))\n",
        "\n",
        "    # input_id_images.append(load_image(upload_images))\n",
        "\n",
        "    id_embed_list = []\n",
        "\n",
        "    for img in input_id_images:\n",
        "        img = np.array(img)\n",
        "        img = img[:, :, ::-1]\n",
        "        faces = analyze_faces(face_detector, img)\n",
        "        if len(faces) > 0:\n",
        "            id_embed_list.append(torch.from_numpy((faces[0]['embedding'])))\n",
        "\n",
        "    if len(id_embed_list) == 0:\n",
        "        raise gr.Error(f\"No face detected, please update the input face image(s)\")\n",
        "\n",
        "    id_embeds = torch.stack(id_embed_list)\n",
        "\n",
        "    generator = torch.Generator(device=device).manual_seed(222)\n",
        "\n",
        "    print(\"Start inference...\")\n",
        "    print(f\"[Debug] Seed: {seed}\")\n",
        "    print(f\"[Debug] Prompt: {prompt}, \\n[Debug] Neg Prompt: {negative_prompt}\")\n",
        "    start_merge_step = int(float(style_strength_ratio) / 100 * num_steps)\n",
        "    if start_merge_step > 15:\n",
        "        start_merge_step = 15\n",
        "    print(start_merge_step)\n",
        "    images = pipe(\n",
        "        prompt=prompt,\n",
        "        width=output_w,\n",
        "        height=output_h,\n",
        "        input_id_images=input_id_images,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=1,\n",
        "        num_inference_steps=num_steps,\n",
        "        start_merge_step=start_merge_step,\n",
        "        generator=generator,\n",
        "        guidance_scale=guidance_scale,\n",
        "        id_embeds=id_embeds,\n",
        "        image=None,\n",
        "        adapter_conditioning_scale=0,\n",
        "        adapter_conditioning_factor=0,\n",
        "    ).images\n",
        "    return images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWnlZ1LAL1nM"
      },
      "source": [
        "# MAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vBCLVol5e-Hw"
      },
      "outputs": [],
      "source": [
        "formats_list = [\".stl\", \".glb\", \".obj\"]\n",
        "\n",
        "# Interface Gradio\n",
        "def swap_to_gallery(images):\n",
        "    return gr.update(value=images, visible=True),  gr.update(visible=False)\n",
        "\n",
        "with gr.Blocks() as iface:\n",
        "    gr.Markdown(\"# Selecionar Método de Geração\")\n",
        "    with gr.Tabs():\n",
        "\n",
        "      with gr.Tab(\"Geração por Imagem\"):\n",
        "          # Entrada de imagem original\n",
        "          with gr.Row():\n",
        "              # input_image = gr.Image(label=\"Imagem de Entrada\", type=\"filepath\")\n",
        "              files = gr.Files(\n",
        "                    label=\"Imagem de Entrada\",\n",
        "                    file_types=[\"image\"]\n",
        "                )\n",
        "              uploaded_files = gr.Gallery(label=\"Sua imagem\", visible=False, columns=5, rows=1, height=200)\n",
        "\n",
        "              # Coluna de estilização\n",
        "              with gr.Column():\n",
        "                  gr.Markdown(\"### Opções de Estilização\")\n",
        "\n",
        "                  # Seleção de estilo\n",
        "                  style_selector = gr.Radio(\n",
        "                      choices=[style[\"name\"] for style in style_list],\n",
        "                      label=\"Selecione o Estilo\",\n",
        "                      value=DEFAULT_STYLE_NAME\n",
        "                  )\n",
        "\n",
        "                  # Parâmetros de geração\n",
        "                  with gr.Row():\n",
        "                      num_steps = gr.Slider(minimum=15, maximum=100, value=50, label=\"Número de Passos\")\n",
        "                      style_strength = gr.Slider(minimum=0, maximum=100, value=20, label=\"Força do Estilo (%)\")\n",
        "\n",
        "                  # Outros parâmetros\n",
        "                  guidance_scale = gr.Slider(minimum=1, maximum=20, value=5, label=\"Guidance Scale\")\n",
        "                  seed = gr.Number(value=0, label=\"Semente Aleatória\")\n",
        "\n",
        "                  # Prompt opcional para estilização\n",
        "                  style_prompt = gr.Textbox(\n",
        "                      label=\"Prompt Opcional para Estilização\",\n",
        "                      placeholder=\"Descreva modificações desejadas\",\n",
        "                      lines=2\n",
        "                  )\n",
        "\n",
        "          # Botão para aplicar estilização\n",
        "          btn_stylize = gr.Button(\"Aplicar Estilização\")\n",
        "\n",
        "          # Saída da imagem estilizada\n",
        "          #styled_image_output = gr.Image(label=\"Imagem Estilizada\", type=\"filepath\")\n",
        "          styled_image_output = gr.Image(label=\"Imagem Estilizada\", visible=False)\n",
        "\n",
        "          files.upload(fn=swap_to_gallery, inputs=files, outputs=[uploaded_files, files])\n",
        "\n",
        "          with gr.Column():\n",
        "              gallery = gr.Image(label=\"Imagem Estilizada\", interactive=False)\n",
        "\n",
        "          # Fluxo de eventos\n",
        "          # 1. Aplicar estilização\n",
        "          btn_stylize.click(\n",
        "              generate_style_image_v2,\n",
        "              inputs=[\n",
        "                  files,                       # upload_images (first)\n",
        "                  style_prompt,                      # prompt\n",
        "                  gr.Textbox(value=\"\", visible=False), # negative_prompt\n",
        "                  style_selector,                    # style_name\n",
        "                  num_steps,                         # num_steps\n",
        "                  style_strength,                    # style_strength_ratio\n",
        "                  gr.Number(value=1),                # num_outputs\n",
        "                  guidance_scale,                    # guidance_scale\n",
        "                  seed                               # seed\n",
        "              ],\n",
        "              outputs=[styled_image_output]\n",
        "          )\n",
        "\n",
        "          styled_image_output.change(\n",
        "              lambda img: img if img is not None else None,\n",
        "              inputs=styled_image_output,\n",
        "              outputs=gallery\n",
        "          )\n",
        "\n",
        "\n",
        "          use_styled = gr.Radio(choices=[\"Imagem Estilizada\", \"Imagem Original\"],\n",
        "                                label=\"Selecione a imagem para gerar o mesh\",\n",
        "                                value=\"Imagem Original\")\n",
        "\n",
        "\n",
        "          mesh_output = gr.Model3D(clear_color=[0.0, 0.0, 0.0, 0.0], label=\"Visualização 3D\")\n",
        "          download_file = gr.File(label=\"Download do Arquivo 3D\")\n",
        "\n",
        "\n",
        "          def generate_mesh(use_styled_val, format_selector, styled_img, original_files):\n",
        "              if use_styled_val == \"Imagem Estilizada\" and styled_img is not None:\n",
        "                  img_to_use = styled_img\n",
        "              else:\n",
        "                  img_to_use = original_files[0] if original_files else None\n",
        "              if img_to_use is None:\n",
        "                  return \"Nenhuma imagem selecionada.\"\n",
        "\n",
        "              return generate_stl_from_image(img_to_use, format_selector)\n",
        "\n",
        "          format_selector = gr.Radio(\n",
        "              choices=[format for format in formats_list],\n",
        "              label=\"Formato de Saída\",\n",
        "              value=formats_list[0]\n",
        "          )\n",
        "\n",
        "          btn_generate_mesh = gr.Button(\"Gerar Mesh\")\n",
        "          btn_generate_mesh.click(\n",
        "              generate_mesh,\n",
        "              inputs=[use_styled, format_selector, styled_image_output, files],\n",
        "              outputs=[mesh_output, download_file]\n",
        "          )\n",
        "\n",
        "\n",
        "      with gr.Tab(\"Gerar por Prompt\"):\n",
        "          prompt_input = gr.Textbox(label=\"Digite o Prompt\")\n",
        "          # prompt_output = gr.File(label=\"Download do STL\")\n",
        "\n",
        "          mesh_prompt_output = gr.Model3D(clear_color=[0.0, 0.0, 0.0, 0.0], label=\"Visualização 3D\")\n",
        "          download_prompt_file = gr.File(label=\"Download da Mesh\")\n",
        "\n",
        "          btn_prompt = gr.Button(\"Gerar Imagem\")\n",
        "\n",
        "          format_selector_prompt = gr.Radio(\n",
        "              choices=[format for format in formats_list],\n",
        "              label=\"Formato de Saída\",\n",
        "              value=formats_list[0]\n",
        "          )\n",
        "          btn_prompt.click(generate_image, inputs=[prompt_input, format_selector_prompt], outputs=[mesh_prompt_output, download_prompt_file])\n",
        "\n",
        "# Lança a interface\n",
        "iface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YZj2lnD7XhYp",
        "BoAL9kHXUcbE",
        "TYW4wzlmU6Al"
      ],
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
